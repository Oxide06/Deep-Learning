{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13dc67e-5b77-41ab-bd35-64150e138485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c415e9d-0408-4fcb-8359-d08b060061fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75a167d-9ab3-44eb-9f18-b35935911d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[[1,2,3,2.5],\n",
    "   [2.0,5.0,-1.0,2.0],\n",
    "   [-1.5,2.7,3.3,-0.8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea4e7514-eba9-48b4-b2e9-29716dfea616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self,n_input,n_neurons,learning_rate=0.1):\n",
    "        self.weights=np.random.randn(n_input,n_neurons)*np.sqrt(1 / n_input)\n",
    "        self.biases=np.zeros((1,n_neurons))\n",
    "        self.learning_rate=0.01\n",
    "    def forward(self,inputs):\n",
    "        self.inputs=inputs\n",
    "        self.output=np.dot(inputs,self.weights)+self.biases\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients of weights and biases\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient for the previous layer\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "    def update(self):\n",
    "        self.weights -= self.learning_rate * self.dweights\n",
    "        self.biases -= self.learning_rate * self.dbiases\n",
    "        \n",
    "class Activation_ReLU:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs=inputs\n",
    "        self.output=np.maximum(0,inputs)\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative of ReLU: If input > 0, pass the gradient, else set to 0\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs=inputs\n",
    "        exp_values= np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "        probs=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.output=probs\n",
    "        \n",
    "        def backward(self, dvalues):\n",
    "        # Initialize gradient array\n",
    "            self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "            for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten softmax output\n",
    "                single_output = single_output.reshape(-1, 1)\n",
    "            # Compute Jacobian matrix\n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Compute gradient\n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self,output,y):\n",
    "        sample_losses=self.forward(output,y)\n",
    "        data_loss=np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self , y_pred,y_true):\n",
    "        samples=len(y_pred)\n",
    "        y_pred_clipped=np.clip(y_pred,1e-7,1-1e-7)\n",
    "        if len(y_true.shape)==1:\n",
    "            correct_confidences=y_pred_clipped[range(samples),y_true]\n",
    "        elif len(y_true.shape)==2:\n",
    "            correct_confidences=np.sum(y_pred_clipped*y_true,axis=1)\n",
    "        negative_log=-np.log(correct_confidences)\n",
    "        return negative_log\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse (categorical), convert them to one-hot encoding\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Compute gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "        \n",
    "class Softmax_CrossEntropy_Loss:\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        return self.loss.calculate(self.activation.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are sparse, convert to one-hot encoding\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(dvalues.shape[1])[y_true]\n",
    "\n",
    "        # Gradient calculation\n",
    "        self.dinputs = dvalues - y_true\n",
    "        self.dinputs = self.dinputs / samples  # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81ac7cf4-898c-4f81-9af3-f086974fc727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At  0  loss : 1.0986119163263455\n",
      "At  100  loss : 1.0986124424221657\n",
      "At  200  loss : 1.0986097220711775\n",
      "At  300  loss : 1.0985542103385197\n",
      "At  400  loss : 1.0963541635113236\n",
      "At  500  loss : 1.0049304412047684\n",
      "At  600  loss : 0.7384816938395264\n",
      "At  700  loss : 0.6673135528113064\n",
      "At  800  loss : 0.6259004648313631\n",
      "At  900  loss : 0.5961774991734413\n",
      "At  1000  loss : 0.5737256717095486\n",
      "At  1100  loss : 0.5563365465871528\n",
      "At  1200  loss : 0.5426577755630128\n",
      "At  1300  loss : 0.531760092565169\n",
      "At  1400  loss : 0.5229694374732644\n",
      "At  1500  loss : 0.5156955376702689\n",
      "At  1600  loss : 0.5094852443288578\n",
      "At  1700  loss : 0.5038203752200124\n",
      "At  1800  loss : 0.4977902145437512\n",
      "At  1900  loss : 0.4894543292632106\n",
      "At  2000  loss : 0.47451195279376307\n",
      "At  2100  loss : 0.44484987914160606\n",
      "At  2200  loss : 0.3910492057846083\n",
      "At  2300  loss : 0.32497248851048194\n",
      "At  2400  loss : 0.25811522251395824\n",
      "At  2500  loss : 0.19889149420334015\n",
      "At  2600  loss : 0.15220615984846853\n",
      "At  2700  loss : 0.11768402116747291\n",
      "At  2800  loss : 0.09279653888563326\n",
      "At  2900  loss : 0.07484753908545184\n"
     ]
    }
   ],
   "source": [
    "layer1 = Layer_Dense(3, 4)\n",
    "activation1 = Activation_ReLU()\n",
    "layer2 = Layer_Dense(4, 3)\n",
    "activation2= Activation_ReLU()\n",
    "layer3 = Layer_Dense(3, 3)\n",
    "loss_activation = Softmax_CrossEntropy_Loss()\n",
    "\n",
    "# Forward pass\n",
    "X = np.array([[1, 2, 3], [0.5, -0.2, 0.1], [0.3, 0.8, -1.2]])  # Example input\n",
    "y = np.array([0, 1, 2])  # Example labels\n",
    "\n",
    "\n",
    "for i in range(3000):\n",
    "    layer1.forward(X)\n",
    "    activation1.forward(layer1.output)\n",
    "    layer2.forward(activation1.output)\n",
    "    activation2.forward(layer2.output)\n",
    "    layer3.forward(activation2.output)\n",
    "    loss= loss_activation.forward(layer3.output,y)\n",
    "    loss_activation.backward(loss_activation.activation.output,y)\n",
    "    layer3.backward(loss_activation.dinputs)\n",
    "    activation2.backward(layer3.dinputs)\n",
    "    layer2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(layer2.dinputs)\n",
    "    layer1.backward(activation1.dinputs)\n",
    "    layer1.update()\n",
    "    layer2.update()\n",
    "    layer3.update()\n",
    "    if i % 100==0:\n",
    "        print(\"At \",i,\" loss :\",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0acfe-d069-4272-81b1-e1ad0991fa9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44a7c4b2-c036-451c-ad6c-356b82a6d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('mushroom_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "855e2a23-6bc5-4af1-8202-a961483be5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97e42004-8ed6-4275-8b68-5c2d5c32ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "   \n",
    "    train_dataset = h5py.File('archive (1)/catvnoncat/train_catvnoncat.h5', \"r\")\n",
    "    test_dataset = h5py.File('archive (1)/catvnoncat/test_catvnoncat.h5', \"r\")\n",
    "\n",
    "    train_x_orig = np.array(train_dataset[\"train_set_x\"][:])  # Training images\n",
    "    train_y = np.array(train_dataset[\"train_set_y\"][:])  # Training labels\n",
    "\n",
    "    test_x_orig = np.array(test_dataset[\"test_set_x\"][:])  # Test images\n",
    "    test_y = np.array(test_dataset[\"test_set_y\"][:])  # Test labels\n",
    "\n",
    "   # Reshape to (1, m_test)\n",
    "\n",
    "    return train_x_orig, train_y, test_x_orig, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "277c08f1-646d-49bc-87a1-f4e55ced91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y= load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a827638-056a-4a0f-b2de-e95671c8512c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 12288)\n",
      "(209, 2)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x_orig.reshape(train_x_orig.shape[0], -1) # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x= test_x_orig.reshape(test_x_orig.shape[0], -1)\n",
    "train_x=train_x/255\n",
    "test_x=test_x/255\n",
    "def one_hot_encode(y, num_classes):\n",
    "    return np.eye(num_classes)[y.reshape(-1)]\n",
    "\n",
    "num_classes = len(np.unique(train_y))\n",
    "train_y = one_hot_encode(train_y, num_classes)\n",
    "test_y = one_hot_encode(test_y, num_classes)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59e49126-3897-4240-85f5-dc974c73cc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At  0  loss : 0.8443547547258125\n",
      "At  100  loss : 0.5090563027193618\n",
      "At  200  loss : 0.42048667977579135\n",
      "At  300  loss : 0.3299193279245668\n",
      "At  400  loss : 0.23798135537098675\n",
      "At  500  loss : 0.1643932646997144\n",
      "At  600  loss : 0.10716146346522688\n",
      "At  700  loss : 0.07870568999024516\n",
      "At  800  loss : 0.055423935599405054\n",
      "At  900  loss : 0.04098350521535681\n",
      "At  1000  loss : 0.0327517561019273\n",
      "At  1100  loss : 0.027012450250726728\n",
      "At  1200  loss : 0.022821720295717763\n",
      "At  1300  loss : 0.019572965512898832\n",
      "At  1400  loss : 0.017023704533922355\n",
      "At  1500  loss : 0.014990290542001345\n",
      "At  1600  loss : 0.013346132688390877\n",
      "At  1700  loss : 0.011983080733260838\n",
      "At  1800  loss : 0.01084157355407832\n",
      "At  1900  loss : 0.00987474446951633\n",
      "At  2000  loss : 0.00905009280239437\n",
      "At  2100  loss : 0.008336454500044149\n",
      "At  2200  loss : 0.0077155222398116955\n",
      "At  2300  loss : 0.007173746682750512\n",
      "At  2400  loss : 0.006690704279983167\n",
      "At  2500  loss : 0.006261437515269172\n",
      "At  2600  loss : 0.005881982866175795\n",
      "At  2700  loss : 0.005540480763241107\n",
      "At  2800  loss : 0.005235508869379899\n",
      "At  2900  loss : 0.004952724267307279\n"
     ]
    }
   ],
   "source": [
    "layer1 = Layer_Dense(12288, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "layer2 = Layer_Dense(64, 2)\n",
    "loss_activation = Softmax_CrossEntropy_Loss()\n",
    "\n",
    "\n",
    "for i in range(3000):\n",
    "    layer1.forward(train_x)\n",
    "    activation1.forward(layer1.output)\n",
    "    layer2.forward(activation1.output)\n",
    "    loss= loss_activation.forward(layer2.output,train_y)\n",
    "    loss_activation.backward(loss_activation.activation.output,train_y)\n",
    "    layer2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(layer2.dinputs)\n",
    "    layer1.backward(activation1.dinputs)\n",
    "    layer1.update()\n",
    "    layer2.update()\n",
    "    if i % 100==0:\n",
    "        print(\"At \",i,\" loss :\",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6acdef99-2571-433d-9b0f-fd83e933067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n"
     ]
    }
   ],
   "source": [
    "layer1.forward(test_x)\n",
    "activation1.forward(layer1.output)\n",
    "layer2.forward(activation1.output)\n",
    "loss_activation.forward(layer2.output,test_y)\n",
    "predictions = np.argmax(loss_activation.activation.output, axis=1)\n",
    "y_true_labels = np.argmax(test_y, axis=1)\n",
    "accuracy = np.mean(predictions == y_true_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc57692f-7355-4520-a338-dbf39e8fe011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: X_train: (105, 4), y_train: (105, 3)\n",
      "Test data shape: X_test: (45, 4), y_test: (45, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "# Features and target\n",
    "X = df[iris.feature_names]\n",
    "y = df['target']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "y_onehot = np.eye(len(np.unique(y)))[y]\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, y_onehot, test_size=0.3)\n",
    "\n",
    "# Show the shapes of the resulting datasets\n",
    "print(f\"Training data shape: X_train: {X_train.shape}, y_train: {Y_train.shape}\")\n",
    "print(f\"Test data shape: X_test: {X_test.shape}, y_test: {Y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b75e4eeb-b737-47dc-b305-f4c652633501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At  0  loss : 1.0762847115622924\n",
      "At  100  loss : 0.9370044543724079\n",
      "At  200  loss : 0.7774894108843673\n",
      "At  300  loss : 0.6733064844066425\n",
      "At  400  loss : 0.6082622443353742\n",
      "At  500  loss : 0.5653644289905496\n",
      "At  600  loss : 0.5277611605660845\n",
      "At  700  loss : 0.4949271551805345\n",
      "At  800  loss : 0.4651064690727723\n",
      "At  900  loss : 0.43566395115776885\n",
      "At  1000  loss : 0.40645339373381945\n",
      "At  1100  loss : 0.37716438065034236\n",
      "At  1200  loss : 0.3481152260886858\n",
      "At  1300  loss : 0.32002823869544583\n",
      "At  1400  loss : 0.2934920019516602\n",
      "At  1500  loss : 0.2689443697777041\n",
      "At  1600  loss : 0.24628013897448747\n",
      "At  1700  loss : 0.22593587862060582\n",
      "At  1800  loss : 0.20768792302406364\n",
      "At  1900  loss : 0.1914421954631651\n",
      "At  2000  loss : 0.1772467427420168\n",
      "At  2100  loss : 0.16475475400580547\n",
      "At  2200  loss : 0.15376911299912857\n",
      "At  2300  loss : 0.14417371754453165\n",
      "At  2400  loss : 0.13574328195124527\n",
      "At  2500  loss : 0.1282926269577896\n",
      "At  2600  loss : 0.12164701656621052\n",
      "At  2700  loss : 0.11568574532001943\n",
      "At  2800  loss : 0.1103136913726746\n",
      "At  2900  loss : 0.10545665890507923\n",
      "At  3000  loss : 0.1011202566106632\n",
      "At  3100  loss : 0.09717171417753123\n",
      "At  3200  loss : 0.09356026052769391\n",
      "At  3300  loss : 0.09025503170656073\n",
      "At  3400  loss : 0.08721485789017216\n",
      "At  3500  loss : 0.0844058580951401\n",
      "At  3600  loss : 0.08180308797434999\n",
      "At  3700  loss : 0.07938566652721299\n",
      "At  3800  loss : 0.07713557494093992\n",
      "At  3900  loss : 0.0750300276675818\n",
      "At  4000  loss : 0.07305809478279587\n",
      "At  4100  loss : 0.07120747253509514\n",
      "At  4200  loss : 0.06946794612570345\n",
      "At  4300  loss : 0.06782942490560799\n",
      "At  4400  loss : 0.06628289758579378\n",
      "At  4500  loss : 0.06482008293507678\n",
      "At  4600  loss : 0.06343560147237443\n",
      "At  4700  loss : 0.062122703475140706\n",
      "At  4800  loss : 0.060876272450718055\n",
      "At  4900  loss : 0.059692591299153795\n",
      "At  5000  loss : 0.05856585534726023\n",
      "At  5100  loss : 0.05749209003136808\n",
      "At  5200  loss : 0.0564640719761958\n",
      "At  5300  loss : 0.05548251425229475\n",
      "At  5400  loss : 0.05454265949934704\n",
      "At  5500  loss : 0.05364188461188586\n",
      "At  5600  loss : 0.05277855020961323\n",
      "At  5700  loss : 0.051949882679394724\n",
      "At  5800  loss : 0.051152301667536475\n",
      "At  5900  loss : 0.050385366702496735\n",
      "At  6000  loss : 0.04964700071227906\n",
      "At  6100  loss : 0.048934911434161114\n",
      "At  6200  loss : 0.04824789189812507\n",
      "At  6300  loss : 0.04758447872713179\n",
      "At  6400  loss : 0.04694333508013664\n",
      "At  6500  loss : 0.046323526770150565\n",
      "At  6600  loss : 0.04572362433315556\n",
      "At  6700  loss : 0.045142771180309654\n",
      "At  6800  loss : 0.044580775677501676\n",
      "At  6900  loss : 0.044036525972795434\n",
      "At  7000  loss : 0.04350866438810078\n",
      "At  7100  loss : 0.04299630769375466\n",
      "At  7200  loss : 0.04249884321365927\n",
      "At  7300  loss : 0.042015566825799024\n",
      "At  7400  loss : 0.04154585973732869\n",
      "At  7500  loss : 0.04108911033479701\n",
      "At  7600  loss : 0.04064498682976269\n",
      "At  7700  loss : 0.0402127578812494\n",
      "At  7800  loss : 0.03979212226937503\n",
      "At  7900  loss : 0.0393825910749594\n",
      "At  8000  loss : 0.038983565520834706\n",
      "At  8100  loss : 0.03859460169003959\n",
      "At  8200  loss : 0.038215319002813726\n",
      "At  8300  loss : 0.037845696165300625\n",
      "At  8400  loss : 0.03748932902990921\n",
      "At  8500  loss : 0.03714346344361871\n",
      "At  8600  loss : 0.036807513496877345\n",
      "At  8700  loss : 0.03648245446983873\n",
      "At  8800  loss : 0.03616646536868702\n",
      "At  8900  loss : 0.03585947610548272\n",
      "At  9000  loss : 0.03556110572937538\n",
      "At  9100  loss : 0.0352701715752842\n",
      "At  9200  loss : 0.034985979977862215\n",
      "At  9300  loss : 0.03470791189702495\n",
      "At  9400  loss : 0.03443543396985457\n",
      "At  9500  loss : 0.03416818679795123\n",
      "At  9600  loss : 0.033905872819081195\n",
      "At  9700  loss : 0.03364823963568527\n",
      "At  9800  loss : 0.0333950684032921\n",
      "At  9900  loss : 0.0331484286603554\n"
     ]
    }
   ],
   "source": [
    "layer1 = Layer_Dense(4, 4)\n",
    "activation1 = Activation_ReLU()\n",
    "layer2 = Layer_Dense(4, 5)\n",
    "activation2= Activation_ReLU()\n",
    "layer3 = Layer_Dense(5, 3)\n",
    "activation3= Activation_ReLU()\n",
    "layer4 = Layer_Dense(3, 3)\n",
    "loss_activation = Softmax_CrossEntropy_Loss()\n",
    "for i in range(10000):\n",
    "    layer1.forward(X_train)\n",
    "    activation1.forward(layer1.output)\n",
    "    layer2.forward(activation1.output)\n",
    "    activation2.forward(layer2.output)\n",
    "    layer3.forward(activation2.output)\n",
    "    activation3.forward(layer3.output)\n",
    "    layer4.forward(activation3.output)\n",
    "    loss= loss_activation.forward(layer4.output,Y_train)\n",
    "    loss_activation.backward(loss_activation.activation.output,Y_train)\n",
    "    layer4.backward(loss_activation.dinputs)\n",
    "    activation3.backward(layer4.dinputs)\n",
    "    layer3.backward(activation3.dinputs)\n",
    "    activation2.backward(layer3.dinputs)\n",
    "    layer2.backward(activation2.dinputs)\n",
    "    activation1.backward(layer2.dinputs)\n",
    "    layer1.backward(activation1.dinputs)\n",
    "    layer1.update()\n",
    "    layer2.update()\n",
    "    layer3.update()\n",
    "    layer4.update()\n",
    "    if i % 100==0:\n",
    "        print(\"At \",i,\" loss :\",loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "680362a2-bf1c-4f10-af35-b9f3879858c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9904761904761905\n"
     ]
    }
   ],
   "source": [
    "layer1.forward(X_train)\n",
    "activation1.forward(layer1.output)\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)\n",
    "layer3.forward(activation2.output)\n",
    "activation3.forward(layer3.output)\n",
    "layer4.forward(activation3.output)\n",
    "loss_activation.forward(layer4.output,Y_train)\n",
    "predictions = np.argmax(loss_activation.activation.output, axis=1)\n",
    "y_true_labels = np.argmax(Y_train, axis=1)\n",
    "accuracy = np.mean(predictions == y_true_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6bbfdb-c36f-4be7-bec7-57244843ac72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
