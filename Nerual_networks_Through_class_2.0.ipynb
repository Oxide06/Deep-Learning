{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec71a1aa-2bf0-4227-9d15-cba877438bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e5f352-0ad9-4d97-8c32-6c4aea75a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_input, n_neurons, learning_rate=0.01):\n",
    "        self.weights = np.random.randn(n_input, n_neurons) * np.sqrt(2 / n_input) #'he' initialization\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights -= self.learning_rate * self.dweights\n",
    "        self.biases -= self.learning_rate * self.dbiases\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "class Loss_CategoricalCrossentropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        return -np.log(correct_confidences)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        self.dinputs = (dvalues - y_true) / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12fee74-37c0-4d30-afb6-62a702c4faa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.0526854596020208\n",
      "Epoch 100, Loss: 0.4271381419922107\n",
      "Epoch 200, Loss: 0.2830556904935301\n",
      "Epoch 300, Loss: 0.2066591619085143\n",
      "Epoch 400, Loss: 0.15178662913429924\n",
      "Epoch 500, Loss: 0.1115204514959502\n",
      "Epoch 600, Loss: 0.08261911097088208\n",
      "Epoch 700, Loss: 0.06228374432446801\n",
      "Epoch 800, Loss: 0.047975468042636776\n",
      "Epoch 900, Loss: 0.037831117779383756\n",
      "Epoch 1000, Loss: 0.030496193091690507\n",
      "Epoch 1100, Loss: 0.02508746571414271\n",
      "Epoch 1200, Loss: 0.021008989008977717\n",
      "Epoch 1300, Loss: 0.017873426281732063\n",
      "Epoch 1400, Loss: 0.015414469229803461\n",
      "Epoch 1500, Loss: 0.013454329217961837\n",
      "Epoch 1600, Loss: 0.011865121716953484\n",
      "Epoch 1700, Loss: 0.010559022704280717\n",
      "Epoch 1800, Loss: 0.009473577891011897\n",
      "Epoch 1900, Loss: 0.008559617881519807\n",
      "Epoch 2000, Loss: 0.007783153157062561\n",
      "Epoch 2100, Loss: 0.0071176397445151035\n",
      "Epoch 2200, Loss: 0.006541528840712967\n",
      "Epoch 2300, Loss: 0.006039982140878107\n",
      "Epoch 2400, Loss: 0.005599591012823653\n",
      "Epoch 2500, Loss: 0.0052111641281084925\n",
      "Epoch 2600, Loss: 0.0048662172242011975\n",
      "Epoch 2700, Loss: 0.004558616446377052\n",
      "Epoch 2800, Loss: 0.004282595959236731\n",
      "Epoch 2900, Loss: 0.0040339954963602345\n",
      "Epoch 3000, Loss: 0.0038091060741654984\n",
      "Epoch 3100, Loss: 0.0036048322216947638\n",
      "Epoch 3200, Loss: 0.003418837659463217\n",
      "Epoch 3300, Loss: 0.003248758231346476\n",
      "Epoch 3400, Loss: 0.0030927435713283594\n",
      "Epoch 3500, Loss: 0.002949370504164267\n",
      "Epoch 3600, Loss: 0.0028169640188640595\n",
      "Epoch 3700, Loss: 0.002694645895451223\n",
      "Epoch 3800, Loss: 0.0025812464144483556\n",
      "Epoch 3900, Loss: 0.002475877962458269\n",
      "Epoch 4000, Loss: 0.002377798797502212\n",
      "Epoch 4100, Loss: 0.002288781935908753\n",
      "Epoch 4200, Loss: 0.00220953131091432\n",
      "Epoch 4300, Loss: 0.002134988680988974\n",
      "Epoch 4400, Loss: 0.00206478824523483\n",
      "Epoch 4500, Loss: 0.001998546295142143\n",
      "Epoch 4600, Loss: 0.0019358837692986096\n",
      "Epoch 4700, Loss: 0.0018766600746589106\n",
      "Epoch 4800, Loss: 0.0018205535043925416\n",
      "Epoch 4900, Loss: 0.0017672730199469317\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 2, 3], [0.5, -0.2, 0.1], [0.3, 0.8, -1.2]])  \n",
    "y = np.array([0, 1, 2])  \n",
    "dense1 = Layer_Dense(3, 5)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(5, 5)\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "dense3 = Layer_Dense(5, 3)\n",
    "activation3 = Activation_Softmax()\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5000):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    dense3.forward(activation2.output)\n",
    "    activation3.forward(dense3.output)\n",
    "    \n",
    "    loss = np.mean(loss_function.forward(activation3.output, y))\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_function.backward(activation3.output, y)\n",
    "    activation3.backward(loss_function.dinputs)\n",
    "    dense3.backward(activation3.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f661c-6f26-4973-a341-b5c1ea69d3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
